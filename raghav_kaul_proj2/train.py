# -*- coding: utf-8 -*-
"""DMProject2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_x7F4MkELAHGn5MrEpkFp36FdPgZfHQM
"""

"""
Name: Raghav Kaul
ASU ID: 1217330751
"""

#Uploading header files
import pandas as pd
import numpy as np
from sklearn.model_selection import RepeatedKFold
from scipy.fftpack import fft
from scipy.stats import entropy
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from scipy.stats import entropy
from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score
from sklearn.model_selection import GridSearchCV 
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
import pickle

#Loading Datasets
my_cols = ["A", "B", "C", "D", "E", "F","G","H","I","J","K","L","M","N","O","P","Q","R","S","T","U","V","W","X","Y","Z","AA","AB","AC","AD","AE"]

#Meal Data
MealData1 = pd.read_csv('mealData1.csv', names = my_cols)
MealData2 = pd.read_csv('mealData2.csv', names = my_cols)
MealData3 = pd.read_csv('mealData3.csv', names = my_cols)
MealData4 = pd.read_csv('mealData4.csv', names = my_cols)
MealData5 = pd.read_csv('mealData5.csv', names = my_cols)

#NoMeal Data
NoMealData1 = pd.read_csv('Nomeal1.csv', names = my_cols)
NoMealData2 = pd.read_csv('Nomeal2.csv', names = my_cols)
NoMealData3 = pd.read_csv('Nomeal3.csv', names = my_cols)
NoMealData4 = pd.read_csv('Nomeal4.csv', names = my_cols)
NoMealData5 = pd.read_csv('Nomeal5.csv', names = my_cols)

#Data Pre-processing 
#1.Removing everything after the 30th column 
#Meal Data
MealData1 = MealData1.loc[:, :'AD']
MealData2 = MealData2.loc[:, :'AD']
MealData3 = MealData3.loc[:, :'AD']
MealData4 = MealData4.loc[:, :'AD']
MealData5 = MealData5.loc[:, :'AD']

#NoMeal Data
NoMealData1 = NoMealData1.loc[:, :'AD']
NoMealData2 = NoMealData2.loc[:, :'AD']
NoMealData3 = NoMealData3.loc[:, :'AD']
NoMealData4 = NoMealData4.loc[:, :'AD']
NoMealData5 = NoMealData5.loc[:, :'AD']

#2. Reversing both datasets for chronologically accurate picture
#Meal Data
MealData1 = MealData1.iloc[:, ::-1]
MealData2 = MealData2.iloc[:, ::-1]
MealData3 = MealData3.iloc[:, ::-1]
MealData4 = MealData4.iloc[:, ::-1]
MealData5 = MealData5.iloc[:, ::-1]

#NoMeal Data
NoMealData1 = NoMealData1.iloc[:, ::-1]
NoMealData2 = NoMealData2.iloc[:, ::-1]
NoMealData3 = NoMealData3.iloc[:, ::-1]
NoMealData4 = NoMealData4.iloc[:, ::-1]
NoMealData5 = NoMealData5.iloc[:, ::-1]

#NoMealData5
#Getting shapes of the resulting dataframes
#print(MealData1.shape)
#print(MealData2.shape)
#print(MealData3.shape)
#print(MealData4.shape)
#print(MealData5.shape)
#print(NoMealData1.shape)
#print(NoMealData2.shape)
#print(NoMealData3.shape)
#print(NoMealData4.shape)
#print(NoMealData5.shape)

#Removing rows with NaN and Empty elements 
#Meal Data
MealData1 = MealData1.dropna()
MealData2 = MealData2.dropna()
MealData3 = MealData3.dropna()
MealData4 = MealData4.dropna()
MealData5 = MealData5.dropna()

#NoMeal Data
NoMealData1 = NoMealData1.dropna()
NoMealData2 = NoMealData2.dropna()
NoMealData3 = NoMealData3.dropna()
NoMealData4 = NoMealData4.dropna()
NoMealData5 = NoMealData5.dropna()

#Meal Data
#print(MealData1.shape)
#print(MealData2.shape)
#print(MealData3.shape)
#print(MealData4.shape)
#print(MealData5.shape)

#print("Total no of meal:: ", MealData1.shape[0]+MealData2.shape[0]+MealData3.shape[0] + MealData4.shape[0]+MealData5.shape[0])

#NoMeal Data
#print(NoMealData1.shape)
#print(NoMealData2.shape)
#print(NoMealData3.shape)
#print(NoMealData4.shape)
#print(NoMealData5.shape)

#print("Total no of Nomeal:: ", NoMealData1.shape[0] + NoMealData2.shape[0] + NoMealData3.shape[0] + NoMealData4.shape[0] + NoMealData5.shape[0])

#print("Total no of records: ", MealData1.shape[0]+MealData2.shape[0]+MealData3.shape[0]+MealData4.shape[0]+MealData5.shape[0] + NoMealData1.shape[0] + NoMealData2.shape[0] + NoMealData3.shape[0] + NoMealData4.shape[0] + NoMealData5.shape[0])

#Concatenating Dataframes 

#Meal Data
List1 = [MealData1, MealData2, MealData3, MealData4, MealData5]  # List of your dataframes
df1 = pd.concat(List1)

#NoMeal Data
#NoMealData1, NoMealData2, NoMealData3, NoMealData4, NoMealData5

#Combined Data
MealList = [MealData1, MealData2, MealData3, MealData4, MealData5, NoMealData1, NoMealData2, NoMealData3, NoMealData4, NoMealData5]  # List of your dataframes
Mealdf = pd.concat(MealList)

#Printing concatenated dataframe
#Mealdf

#Making copy of whatever dataframe 
MealDatacopy = Mealdf


#Extracting Features and creating Feature Matrix
#1. Moving Average 
updatedfeatureMatrix = pd.DataFrame()
#Calculting averages for discrete 30 minute intervals 
meanFrame = pd.DataFrame()
for i in range(0,30,6):
  meanFrame['Mean ' + str(i)+"-"+str(i + 6)] = MealDatacopy.iloc[:, i :i + 6].mean(axis = 1)
#meanFrame

#Inserting features in feature matrix
updatedfeatureMatrix['mean0-6']= meanFrame['Mean 0-6']
updatedfeatureMatrix['mean6-12']= meanFrame['Mean 6-12']
updatedfeatureMatrix['mean12-18']= meanFrame['Mean 12-18']
updatedfeatureMatrix['mean18-24']= meanFrame['Mean 18-24']
updatedfeatureMatrix['mean24-30']= meanFrame['Mean 24-30']

#Displaying updated feature matrix
updatedfeatureMatrix

#2.Maximum difference [30-Minute Intervals]
#Calculate maximum difference of each row 
velocityFrame = pd.DataFrame() 
for i in range(0,25):
  velocityFrame['Velocity '+ str(i+1)+"-"+ str(i+5)] = ((MealDatacopy.iloc[: , i + 5]) - (MealDatacopy.iloc[: , i]))
#velocityFrame

#Inserting features in the feature matrix
updatedfeatureMatrix['maximumVelocity']= velocityFrame.max(axis = 1)
updatedfeatureMatrix

#3.Entropy 
#Function to calculate entropy of each row
def calculateEntropy(series):
    numberofSeries = series.value_counts()
    entropyvalues = entropy(numberofSeries)  
    return entropyvalues

entropyTest = pd.DataFrame()
entropyTest['Entropy'] = MealDatacopy.apply(lambda row: calculateEntropy(row), axis=1) 
#entropyTest

#Inserting values in updated feature matrix
updatedfeatureMatrix['Entropy'] = MealDatacopy.apply(lambda row: calculateEntropy(row), axis=1) 
updatedfeatureMatrix

#4. Feature of Covariation 
feature_COV = pd.DataFrame()
feature_COV["COV"] = MealDatacopy.mean(axis= 1) / MealDatacopy.std(axis= 1)
#feature_COV

#Adding COV into updated feature matrix 
updatedfeatureMatrix['COV'] = feature_COV
updatedfeatureMatrix

################PROVIDING FEATURE MATRIX TO PCA ###########################
#Standardizing feature matrix
updatedfeatureMatrix = StandardScaler().fit_transform(updatedfeatureMatrix)

#Taking Top 5 Components for PCA
pca = PCA(n_components = 5)
principalComponents = pca.fit(updatedfeatureMatrix)
#print(principalComponents.components_)

principalComponentsTrans = pca.fit_transform(updatedfeatureMatrix)
pc5Matrix = pd.DataFrame(data = principalComponentsTrans, columns = ['Principal Component 1', 'Principal Component 2','Principal Component 3', 'Principal Component 4','Principal Component 5'])
pc5Matrix

#print(principalComponents.explained_variance_ratio_.cumsum())

#Assigning Class Labels  
#pd.set_option('display.max_rows', None)
#pd.set_option('display.max_columns', None)
#pd.set_option('display.width', None)
#pd.set_option('display.max_colwidth', -1)


mealen = MealData1.shape[0]+MealData2.shape[0]+MealData3.shape[0] + MealData4.shape[0]+MealData5.shape[0]
#print(mealen)

nomealen = NoMealData1.shape[0] + NoMealData2.shape[0] + NoMealData3.shape[0] + NoMealData4.shape[0] + NoMealData5.shape[0]
#print(nomealen)

totalen =  MealData1.shape[0]+MealData2.shape[0]+MealData3.shape[0] + MealData4.shape[0]+MealData5.shape[0] + NoMealData1.shape[0] + NoMealData2.shape[0] + NoMealData3.shape[0] + NoMealData4.shape[0] + NoMealData5.shape[0]
#print(totalen)

#print(totalen - mealen)

mealabel = pd.DataFrame(index=np.arange(mealen), columns=np.arange(1))
nomealabel = pd.DataFrame(index=np.arange(nomealen), columns=np.arange(1))

classLabel = pd.DataFrame()

mealabel[0] = 1
mealabel

nomealabel[0] = 0
nomealabel

labelist = [mealabel, nomealabel]  # List of your dataframes
classLabel = pd.concat(labelist)

classLabel = classLabel.reset_index(drop=True)
#classLabel

#Adding class labels to the pc5 matrix 

#print(pc5Matrix)
#print(classLabel)

#SVM Classifier 
TestScore = []
matrix = [[0,0],[0,0]]
pScore = []
recall = []
F1Score = []
d=[]


X = pc5Matrix.to_numpy()
y = classLabel.to_numpy()

#X_train,X_test,y_train,y_test = train_test_split(X, y,test_size = 0.2)
#X_train.shape,X_test.shape,y_train.shape,y_test.shape

svclassifier = SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=False)


#Implementing KFold Cross Validation
kf = KFold(n_splits = 20, shuffle = True) 

for train_index, test_index in kf.split(X):
      #print("Train:", train_index, "Validation:",test_index)
      X_train, X_test = X[train_index], X[test_index] 
      y_train, y_test = y[train_index], y[test_index]

      svclassifier.fit(X_train, y_train)

      p = svclassifier.predict(X_test)

      matrix = matrix + confusion_matrix(y_test, p)
      pScore.append(precision_score(y_test, p))
      recall.append(recall_score(y_test, p))
      F1Score.append(f1_score(y_test, p))

      TestScore.append(svclassifier.score(X_test,y_test))

Accuracy = np.max(TestScore)
precision=np.max(pScore)
Recall=np.max(recall)
F1score=np.max(F1Score)
ConfusionMatrix = matrix

#Displaying Metrics
print("Accuracy", Accuracy)
print("F1Score", F1score)
print("Precision", precision)
print("Recall", Recall)
#print(ConfusionMatrix)

#Dumping model in pickle file format
filename = 'finalized_model.pkl'
pickle.dump(svclassifier, open(filename, 'wb'))